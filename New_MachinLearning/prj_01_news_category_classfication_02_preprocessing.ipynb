{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prj_01_news_category_classfication_02_preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJo4cRSusJzpOtZjqCYUef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sera0911/asia_ai_study/blob/main/New_MachinLearning/prj_01_news_category_classfication_02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmeZ3P036FqO"
      },
      "source": [
        "## 네이버 기사 헤드라인을 크롤링한 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QgYiH4Hd1WA"
      },
      "source": [
        "! pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3NDjCTf5i4y"
      },
      "source": [
        "#모듈 불러오기\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p3OZjQ57PtB"
      },
      "source": [
        "pd.set_option('display.unicode.east_asian_width', True)  #줄맞춤을 보기에 간결하게 해준다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvokWE6k6hYw",
        "outputId": "9ad19fa3-78f2-4417-ec45-9ea1788d8127"
      },
      "source": [
        "df = pd.read_csv('/content/datasets/naver_news_titles_210616.csv', index_col=0) \n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                title  category\n",
            "0    김총리  새 거리두기 방안  일상에 큰 변화 될 것                    Politics\n",
            "1     백신 유급휴가비  지원한다 법개정안 복지위 통과                   Politics\n",
            "2   쇄신   변화  강조한 송영길  개혁 속도조절  국민공감대 우선 ......  Politics\n",
            "3    소통수석  열린자세로  의 중요 이웃국 역할할 것  종합 ......       Politics\n",
            "4             큐어백  백신 도입     한국  생산 거점                    Politics\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 28309 entries, 0 to 28308\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   title     28309 non-null  object\n",
            " 1   category  28309 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 663.5+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80-3JjFQjdX7",
        "outputId": "54833e4e-6fd3-44bc-82da-ba0e2bf4d964"
      },
      "source": [
        "#중복된 기사가 있을 수 있어서 중복된 데이터들은 삭제해준다\n",
        "\n",
        "col_dup = df['title'].duplicated()  #duplicated=중복이라면 true\n",
        "print(col_dup)\n",
        "sum_dup = df.title.duplicated().sum()  #중복 개수 확인\n",
        "print(sum_dup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        False\n",
            "1        False\n",
            "2        False\n",
            "3        False\n",
            "4        False\n",
            "         ...  \n",
            "28304    False\n",
            "28305    False\n",
            "28306    False\n",
            "28307    False\n",
            "28308    False\n",
            "Name: title, Length: 28309, dtype: bool\n",
            "1234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGZQ4UzFj_Ad",
        "outputId": "82a4c1a4-ba81-4a34-8ad4-c70b4c6f381f"
      },
      "source": [
        "df = df.drop_duplicates(subset=['title'])  #타이틀이 중복이라면 중복된 row 제거\n",
        "sum_dup = df.title.duplicated().sum()  #제거 후 다시 중복 개수 확인\n",
        "print(sum_dup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cWzA3wJkVt-"
      },
      "source": [
        "#중복이 제거되면 데이터프레임 인덱스가 빠지게 되어 다시 정렬\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)  #원본을 바꿔주기 위해 둘 다 true로 준다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9tPFbdH7qMZ"
      },
      "source": [
        "#데이터를 나눠준다(피쳐, 타켓값으로)\n",
        "X = df['title']\n",
        "Y = df['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LGXUtDV9I6i"
      },
      "source": [
        "#### Y 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9a9FjL07512",
        "outputId": "442c2048-d795-480e-961e-cf111bbb010e"
      },
      "source": [
        "#Y(타겟)는 문자열로 들어가있기에 라벨로 바꿔준다\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "labeled_Y = encoder.fit_transform(Y)  #데이터를 라벨로 바꿔준다 (문자열 6가지를 같은 문자끼리 라벨로 묶어준다)\n",
        "label = encoder.classes_  #인코더에 등록된 클래스 조회\n",
        "print(label)  #카테고리 이름확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Culture' 'Economic' 'IT' 'Politics' 'Social' 'World']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkif4Ulnbkac"
      },
      "source": [
        "#라벨 순서를 기억해야 해서 따로 저장해 준다\n",
        "#원래 숫자를 파일로 저장하면 문자열로 저장되지만 ,pickle을 사용하면 데이터 형태 그대로 저장된다(문자 or 숫자 or plt등등)\n",
        "import pickle \n",
        "\n",
        "with open('/content/datasets/category_encoder.pickle', 'wb') as f:  #f란 이름으로 열어준다\n",
        "    pickle.dump(encoder, f)  #dump= 토큰을 f에 저장시켜준다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1XFs5Tq8MlX",
        "outputId": "4646e214-fb6f-473b-9c5e-b3750b039fc8"
      },
      "source": [
        "print(labeled_Y)  #라벨화 시킨 카테고리가 잘변환됐는지 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 3 3 ... 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdITa5LE8Tw7",
        "outputId": "fa58d4fd-8e1b-4b80-9012-97d041863ab0"
      },
      "source": [
        "#라벨이 붙은 걸 원핫인코딩으로 변환 \n",
        "\n",
        "onehot_Y = to_categorical(labeled_Y)  #희소행렬화\n",
        "print(onehot_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Q3c6-Q9Lek"
      },
      "source": [
        "#### X 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-iTuPD_9BjJ",
        "outputId": "5951d0d7-d23b-4301-a242-ddbbef820b7e"
      },
      "source": [
        "#자연어인 X를 전처리해준다(한글 형태소 분류기 사용 - Okt)\n",
        "\n",
        "okt = Okt()\n",
        "print(type(X))\n",
        "print(X[0])  #원본 기사 제목\n",
        "okt_X = okt.morphs(X[0])  #첫번째 기사 제목만 넣어보기, okt.morphs= 텍스트를 형태소 단위로 나누는데, 이때 각 단어에서 어간을 추출\n",
        "print(okt_X) \n",
        "okt_X = okt.pos(X[0])  #각 단어의 품사를 태깅해준다\n",
        "print(okt_X)  #okt화 시킨 것(리스트로 들어가진다)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "총리 거리 두기 방안 일상 변화\n",
            "['총리', '거리', '두기', '방안', '일상', '변화']\n",
            "[('총리', 'Noun'), ('거리', 'Noun'), ('두기', 'Noun'), ('방안', 'Noun'), ('일상', 'Noun'), ('변화', 'Noun')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtAPdq87i1z9"
      },
      "source": [
        "1. okt = Okt() - 객체 생성  \n",
        "2. okt.morphs() - 텍스트를 형태소 단위로 나눈다. 옵션으로는 norm과 stem이 있다\n",
        "- norm은 normalize의 약자로 문장을 정규화하는 역할\n",
        "- stem은 각 단어에서 어간을 추출하는 기능\n",
        "3. okt.nouns() - 텍스트에서 명사만 뽑아낸다.\n",
        "4. okt.phrases() - 텍스트에서 어절을 뽑아낸다.\n",
        "5. okt.pos() - 각 품사를 태깅하는 역할.\n",
        "- 품사를 태깅한다는 것은 주어진 텍스트를 형태소 단위로 나누고, 나눠진 각 형태소를 그에 해당하는 품사와 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN9ROUnABakm",
        "outputId": "285a9b95-95c0-40b7-f1a3-07d4f61eb8c3"
      },
      "source": [
        "# for문을 사용해서 X전체 텍스트를 형태소 단위로 분리시키기 \n",
        "\n",
        "for i in range(len(X)):\n",
        "    X[i] = okt.morphs(X[i])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        [김, 총리, 새, 거리, 두기, 방안, 일상, 에, 큰, 변화, 될, 것]...\n",
            "1        [백신, 유급, 휴가, 비, 지원, 한, 다, 법, 개정안, 복지, 위, 통과]...\n",
            "2        [쇄신, 변화, 강조, 한, 송영길, 개혁, 속도, 조절, 국민, 공감, 대, 우선...\n",
            "3        [소통, 수석, 열린, 자세, 로, 의, 중요, 이웃, 국, 역할, 할, 것, 종합...\n",
            "4                 [큐어, 백, 백신, 도입, 한국, 생산, 거점]\n",
            "                               ...                        \n",
            "27070    [2만, 4000년, 간, 죽지, 않은, 좀비, 가, 시베리아, 에서, 나타났다]...\n",
            "27071    [PLAY, IT, 갤럭시, 북, 경험, 키, 보드, 에서도, 블루투스, 키, 보드...\n",
            "27072    [PLAY, IT, 카카오, 판, 클럽, 하우스, 음, 체험, 기, 작, 지만, 큰...\n",
            "27073    [PLAY, IT, 홈쇼핑, 고객, 정착, 가능할까, CJ, 온스타일, 이용, 해보...\n",
            "27074    [PLAY, IT, 크롬, 잡겠다는, 네이버, 웨일, 신, 기능, 사용, 해보니]...\n",
            "Name: title, Length: 27075, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeGuQejZCSM2",
        "outputId": "acfbc1fd-d74b-4b0f-9af8-e3448d0df274"
      },
      "source": [
        "# 형태소로 잘라 낸 뒤, 의미 없는 조사, 접속사, 감탄사 같은 단어들은 제거하기 위해 이 단어들을 모아놓은 파일 가져오기\n",
        "\n",
        "stopwords = pd.read_csv('/content/datasets/stopwords.csv')\n",
        "print(stopwords.head(30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Unnamed: 0  stopword\n",
            "0            0        아\n",
            "1            1        휴\n",
            "2            2    아이구\n",
            "3            3    아이쿠\n",
            "4            4    아이고\n",
            "5            5        어\n",
            "6            6        나\n",
            "7            7      우리\n",
            "8            8      저희\n",
            "9            9      따라\n",
            "10          10      의해\n",
            "11          11        을\n",
            "12          12        를\n",
            "13          13        에\n",
            "14          14        의\n",
            "15          15        가\n",
            "16          16      으로\n",
            "17          17        로\n",
            "18          18      에게\n",
            "19          19    뿐이다\n",
            "20          20  의거하여\n",
            "21          21  근거하여\n",
            "22          22  입각하여\n",
            "23          23  기준으로\n",
            "24          24    예하면\n",
            "25          25      예를\n",
            "26          26      들면\n",
            "27          27    들자면\n",
            "28          28        저\n",
            "29          29      소인\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5EmCaQMUxr0",
        "outputId": "2c5ad666-edc4-4604-b981-8301077c8622"
      },
      "source": [
        "#stopwords로 불필요한 단어 제거하기\n",
        "\n",
        "words = []  #빈리스트 생성\n",
        "for word in okt_X:\n",
        "    if word not in list(stopwords['stopword']):  #단어가 stopwords안에 있지 않다면 words 리스트에 추가하기\n",
        "        words.append(word)\n",
        "\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('김', 'Noun'), ('총리', 'Noun'), ('새', 'Noun'), ('거리', 'Noun'), ('두기', 'Noun'), ('방안', 'Noun'), ('일상', 'Noun'), ('에', 'Josa'), ('큰', 'Verb'), ('변화', 'Noun'), ('될', 'Verb'), ('것', 'Noun')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HG5rPgmUydy",
        "outputId": "e97d0517-5a5d-4d75-96e1-e6b50d13572e"
      },
      "source": [
        "#한 글자인 단어와, stopwords로 불필요한 단어 제거하기\n",
        "\n",
        "for i in range(len(X)):  #i는 각각의 문장들\n",
        "    result = []\n",
        "    for j in range(len(X[i])):  #j는 각 문장 안 단어들\n",
        "        if len(X[i][j]) > 1:  #한글자인 단어들은 if문 안에 못들어간다\n",
        "            if X[i][j] not in list(stopwords['stopword']):  #그리고 stopwords에 없는 단어들만 들어간다\n",
        "                result.append(X[i][j])  #통과된 텍스트를 빈 리스트에 넣어주기\n",
        "    X[i] = ' '.join(result)  #문장 별 리스트 요소들을 join으로 이어준다\n",
        "    \n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                            총리 거리 두기 방안 일상 변화\n",
            "1                     백신 유급 휴가 지원 개정안 복지 통과\n",
            "2           쇄신 변화 강조 송영길 개혁 속도 조절 국민 공감\n",
            "3                  소통 수석 열린 자세 중요 이웃 역할 종합\n",
            "4                            큐어 백신 도입 한국 생산 거점\n",
            "                               ...                        \n",
            "27070          2만 4000년 죽지 않은 좀비 시베리아 나타났다\n",
            "27071    PLAY IT 갤럭시 경험 보드 에서도 블루투스 보드 트리오 500 보니...\n",
            "27072                 PLAY IT 카카오 클럽 하우스 체험 차이\n",
            "27073    PLAY IT 홈쇼핑 고객 정착 가능할까 CJ 온스타일 이용 해보니...\n",
            "27074    PLAY IT 크롬 잡겠다는 네이버 웨일 기능 사용 해보니...\n",
            "Name: title, Length: 27075, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB8dwZfQV3Ln",
        "outputId": "dcd5a1cb-e8a8-48ed-98f9-1323f3b6d12d"
      },
      "source": [
        "#토크나이징 작업하기(숫자로 라벨화 작업)\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(X)  #토크나이저에서는 fit_transform이 아닌 fit_on_texts를 사용하여 변환한다, 각 단어들에게 라벨을 붙여준다(token 번호만 가지고 있는다)\n",
        "tokened_X = token.texts_to_sequences(X)  #texts_to_sequences = 단어를 시퀀스형태로 변환하기(문장변화는 여기서 일어난다)\n",
        "\n",
        "print(tokened_X[0])  #첫줄만 확인하기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[281, 74, 138, 873, 603, 126]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dAi3ljWZYXV"
      },
      "source": [
        "#토큰 저장하기(학습한 문장을 라벨을 붙였기에 그대로 저장해준다)\n",
        "\n",
        "with open('/content/datasets/news_token.pickle', 'wb') as f:  #f란 이름으로 열어준다\n",
        "    pickle.dump(token, f)  #dump= 토큰을 f에 저장시켜준다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L10h0A04bH0s",
        "outputId": "09492c05-1d2d-494f-a921-bbeb4683bcb6"
      },
      "source": [
        "wordsize = len(token.word_index) + 1   #word_index = 각 단어 별 붙여진 번호를 볼 수 있다, len을 써서 붙인 단어의 개수를 출력함(0이 없어서 0을 추가하기 위해서 1을 더해준다)\n",
        "print(wordsize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzI3xUSXcmu-",
        "outputId": "386a45fb-5d81-4239-9084-e40a12d6e20e"
      },
      "source": [
        "#문장별로 단어의 갯수가 달라서 맞춰주기\n",
        "\n",
        "max = 0\n",
        "for i in range(len(tokened_X)):\n",
        "    if max < len(tokened_X[i]):  #문장을 하나하나 꺼내서 길이를 보면서 가장 큰 문장을 찾아준다\n",
        "        max = len(tokened_X[i])  #가장 긴 문장을 다시 max값으로 지정\n",
        "\n",
        "print(max)  #최대 문장 길이= 27"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb9AeryHeIGI",
        "outputId": "25d895fc-0d98-4a0b-a200-f0179b4607e5"
      },
      "source": [
        "#16개의 길이를 맞추기 위해 짧은 문장은 앞쪽에 0으로 채워준다\n",
        "\n",
        "X_pad = pad_sequences(tokened_X, max)  #pad_sequences= max가 안되는 문장을 0으로 채워준다(앞쪽에)\n",
        "print(X_pad[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0   281    74   138\n",
            "    873   603   126]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     2  4851  1001   105\n",
            "   1953  2506   189]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0  2507   126  1030    66   636   219\n",
            "   2825     7  1245]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0   923  1803  1804  3229  1421\n",
            "   1547  1095     4]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0  1805     2   312\n",
            "     11   156  2661]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0    66    19  1139 13780  3725  1139\n",
            "   1059  7004 13781]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0 10293    47  3011\n",
            "    160  4376 10294]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0    48   168\n",
            "    227  2150  1489]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0  1490  1660     1  8311  3012   834   377\n",
            "   1422  4852 10295]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0   479    99   449  4006\n",
            "  13782   461  3230]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzRo9MtthM99"
      },
      "source": [
        "#### 훈련, 결과 데이터 나눠서 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEt5SP6weWaP",
        "outputId": "eca3bf7f-7ea3-46d0-e1a2-79a7937f929c"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_pad, onehot_Y, test_size=0.1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24367, 27)\n",
            "(2708, 27)\n",
            "(24367, 6)\n",
            "(2708, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8JjSk0Lhp9g",
        "outputId": "4c094a41-ed35-4ea4-9723-f6135741254c"
      },
      "source": [
        "#나눈 데이터 저장하기\n",
        "\n",
        "xy = X_train, X_test, Y_train, Y_test\n",
        "np.save('/content/datasets/news_data_max_{}_size_{}'.format(max, wordsize), xy)  #max, wordsize는 전체 뉴스기사를 넣었을 땐 달라질 수 있기에 {}안에 넣어준다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}